"""
Discord bot with ChatGPT-style replies via local Ollama (free, local LLM).

Setup:
1) Install deps: pip install -U discord.py requests
2) –°–æ–∑–¥–∞–π—Ç–µ –±–æ—Ç–∞ –∏ –≤–æ–∑—å–º–∏—Ç–µ —Ç–æ–∫–µ–Ω: https://discord.com/developers/applications
3) –ó–∞–¥–∞–π—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é —Å —Ç–æ–∫–µ–Ω–æ–º:
   PowerShell:  $env:DISCORD_BOT_TOKEN="your_token_here"
   CMD:         set DISCORD_BOT_TOKEN=your_token_here
4) –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Ollama –∏ –º–æ–¥–µ–ª—å (–ø—Ä–∏–º–µ—Ä ‚Äî llama3):
   - –°–∫–∞—á–∞—Ç—å: https://ollama.com/download
   - –ü–æ–¥—Ç—è–Ω—É—Ç—å –º–æ–¥–µ–ª—å: ollama pull llama3
   (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –æ–∂–∏–¥–∞–µ—Ç—Å—è —Å–µ—Ä–≤–µ—Ä –Ω–∞ http://127.0.0.1:11434)
   –ú–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å: OLLAMA_URL, OLLAMA_MODEL.
5) –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞: python discord_bot.py

–ö–æ–º–∞–Ω–¥—ã (–ø—Ä–µ—Ñ–∏–∫—Å "!"):
- !ping                -> Pong
- !roll [max]          -> —Å–ª—É—á–∞–π–Ω–æ–µ 1..max (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 100)
- !gpt <—Ç–µ–∫—Å—Ç>         -> –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω—ã–π Ollama (—Å –∫—Ä–∞—Ç–∫–æ–π –∏—Å—Ç–æ—Ä–∏–µ–π –≤ –∫–∞–Ω–∞–ª–µ)
"""

import asyncio
import os
import random
from collections import defaultdict, deque
from typing import Deque, Dict, List, Tuple

import discord
import requests
from discord.ext import commands

Message = Tuple[str, str]  # (role, content)


def make_bot() -> commands.Bot:
    intents = discord.Intents.default()
    intents.message_content = True
    bot = commands.Bot(command_prefix="!", intents=intents)

    history: Dict[int, Deque[Message]] = defaultdict(lambda: deque(maxlen=10))
    ollama_url = os.getenv("OLLAMA_URL", "http://127.0.0.1:11434/api/generate")
    ollama_model = os.getenv("OLLAMA_MODEL", "llama3")

    @bot.event
    async def on_ready() -> None:
        print(f"Logged in as {bot.user} (id={bot.user.id})")

    @bot.command(name="ping")
    async def ping(ctx: commands.Context) -> None:
        await ctx.reply("Pong!")

    @bot.command(name="roll")
    async def roll(ctx: commands.Context, max_value: int = 100) -> None:
        if max_value < 1:
            await ctx.reply("Max must be >= 1.")
            return
        value = random.randint(1, max_value)
        await ctx.reply(f"üé≤ {value} (1-{max_value})")

    @bot.command(name="gpt")
    async def gpt(ctx: commands.Context, *, text: str | None = None) -> None:
        if not text:
            await ctx.reply("Usage: !gpt <message>")
            return

        chan_hist = history[ctx.channel.id]
        chan_hist.append(("user", text))
        prompt = build_prompt(chan_hist)

        try:
            reply_text = await asyncio.to_thread(
                query_ollama, prompt, model=ollama_model, url=ollama_url
            )
        except Exception as exc:  # noqa: BLE001
            await ctx.reply(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ –º–æ–¥–µ–ª–∏: {exc}")
            return

        chan_hist.append(("assistant", reply_text))
        await ctx.reply(reply_text[:1800])

    return bot


def build_prompt(history: Deque[Message]) -> str:
    """Construct a simple conversational prompt."""
    parts: List[str] = [
        "You are a helpful assistant in a Discord chat. Keep answers concise."
    ]
    for role, content in history:
        prefix = "User" if role == "user" else "Assistant"
        parts.append(f"{prefix}: {content}")
    parts.append("Assistant:")
    return "\n".join(parts)


def query_ollama(prompt: str, model: str, url: str) -> str:
    """
    Call local Ollama HTTP API for a completion.
    Requires Ollama running locally with the specified model pulled.
    """
    payload = {"model": model, "prompt": prompt, "stream": False}
    resp = requests.post(url, json=payload, timeout=60)
    resp.raise_for_status()
    data = resp.json()
    text = data.get("response") or ""
    return text.strip()


def main() -> None:
    token = os.getenv("DISCORD_BOT_TOKEN")
    if not token:
        raise SystemExit("Set DISCORD_BOT_TOKEN environment variable with your bot token.")
    bot = make_bot()
    bot.run(token)


if __name__ == "__main__":
    main()
